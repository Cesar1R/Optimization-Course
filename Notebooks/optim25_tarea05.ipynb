{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de Optimización \n",
    "## Tarea 5\n",
    "\n",
    "| Descripción:                         | Fechas               |\n",
    "|--------------------------------------|----------------------|\n",
    "| Fecha de publicación del documento:  | **Marzo 10, 2025**   |\n",
    "| Fecha límite de entrega de la tarea: | **Marzo 16, 2025**   |\n",
    "\n",
    "\n",
    "### Indicaciones\n",
    "\n",
    "- Envie el notebook con los códigos y las pruebas realizadas de cada ejercicio.\n",
    "- Si se requiren algunos scripts adicionales para poder reproducir las pruebas,\n",
    "  agreguelos en un ZIP junto con el notebook.\n",
    "- Genere un PDF del notebook y envielo por separado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuelto por Cesar Amilkar Rivera Covarrubias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from funciones_prop import *\n",
    "\n",
    "eps = np.finfo('float').eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1 (5 puntos)\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal descrito en el Algoritmo 1 \n",
    "de Clase 14 usando la fórmula de Hestenes-Stiefel para cálcular el factor $\\beta_{k+1}$ \n",
    "que se requiere para calcular la siguiente dirección $\\mathbf{d}_{k+1}$:\n",
    "\n",
    "$$ \\mathbf{y}_k =  \\nabla f_{k+1}-\\nabla f_{k} $$\n",
    "$$ \\beta_{k+1} =   \\frac{\\nabla f_{k+1}^\\top\\mathbf{y}_k }{\\mathbf{d}_{k}^\\top\\mathbf{y}_k}  $$\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "1. Escriba la función que implemente el algoritmo. \n",
    "\n",
    "- La función debe recibir como argumentos $\\mathbf{x}_0$, la función $f$ y \n",
    "  su gradiente, el número máximo de iteraciones $N$, la tolerancia $\\tau$, y los\n",
    "  parámetros para el algoritmo de backtracking: factor $\\rho$, la constante $c_1$\n",
    "  para la condición de descenso suficiente, la constante $c_2$ para la condición\n",
    "  de curvatura, y el máximo número de iteraciones $N_b$.\n",
    "- Agregue al algoritmo un contador\n",
    "  $nr$ que se incremente cada vez que se aplique el reinicio, es decir, cuando\n",
    "  se hace $\\beta_{k+1}=0$.\n",
    "   \n",
    "- Para calcular el tamaño de paso $\\alpha_k$ use el algoritmo de backtracking\n",
    "  usando las condiciones de Wolfe con el valor inicial $\\alpha_{ini}=1$.\n",
    "\n",
    "- Haga que la función devuelva el último punto  $\\mathbf{x}_k$, \n",
    "  el último gradiente $\\mathbf{g}_k$, el número de iteraciones $k$ \n",
    "  y una variable binaria $bres$ que indique si se cumpli\\'o el criterio\n",
    "  de paro ($bres=True$) o si el algoritmo terminó por\n",
    "  iteraciones ($bres=False$), y el contador $bres$.\n",
    "\n",
    "2. Pruebe el algoritmo usando la siguientes funciones con los puntos iniciales dados:\n",
    "\n",
    "\n",
    "**Función de cuadrática 1:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$ y\n",
    "\n",
    "$f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}\\mathbf{x} - \\mathbf{b}^\\top\\mathbf{x}$,\n",
    "donde $\\mathbf{A}$ y $\\mathbf{b}$.\n",
    "\n",
    "$$ \\mathbf{A} = \\begin{bmatrix}  \n",
    "     2 & -1      &        &        &     \\\\ \n",
    "    -1 &  2      & -1 &        &     \\\\ \n",
    "       &  \\ddots & \\ddots & \\ddots &     \\\\ \n",
    "       &         & -1 & 2      & -1  \\\\ \n",
    "       &         &        & -1     &  2  \n",
    "  \\end{bmatrix},  \\qquad  \n",
    "  \\mathbf{b} = \\begin{pmatrix}  \n",
    "     1  \\\\ \n",
    "     0  \\\\ \n",
    "    \\phantom{\\ddots} \\vdots \\phantom{\\ddots} \\\\ \n",
    "     0  \\\\ \n",
    "     1   \n",
    "  \\end{pmatrix}. \n",
    "$$\n",
    "\n",
    "\n",
    "- Use $\\mathbf{x}_0 = \\mathbf{b} \\in \\mathbb{R}^{n}$  para $n=101$ y $n=1001$.\n",
    "\n",
    "\n",
    "**Función de Beale :** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (1.5-x_1 + x_1x_2)^2 + (2.25 - x_1 + x_1x_2^2)^2 + (2.625 - x_1 + x_1x_2^3)^2.$$\n",
    "- $\\mathbf{x}_0 = (2,3)$  \n",
    "   \n",
    "\n",
    "**Función de Himmelblau:** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2. $$\n",
    "- $\\mathbf{x}_0 = (2,4)$\n",
    "\n",
    "\n",
    "\n",
    "**Función de Rosenbrock:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1-x_i)^2 \\right]\n",
    "\\quad n\\geq 2.$$\n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0)\\in \\mathbb{R}^{2}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{20}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{40}$ \n",
    "\n",
    "\n",
    "3. Fije el máximo de iteraciones $N=50000$, $\\tau = \\sqrt{n}\\epsilon_m^{1/3}$, donde $n$ es la dimensión\n",
    "   de la variable $\\mathbf{x}$ y $\\epsilon_m$ es el épsilon máquina. \n",
    "   Para backtracking use $a_{ini}=1$, $\\rho=0.75$, $c_1=0.001$, $c_2=0.9$ y $N_b=100$ iteraciones máximas.\n",
    "   \n",
    "4. Para cada función de prueba imprima\n",
    "   \n",
    "- la dimensión $n$,\n",
    "- $f(\\mathbf{x}_0)$,\n",
    "- el número $k$ de iteraciones realizadas,\n",
    "- $f(\\mathbf{x}_k)$,\n",
    "- las primeras y últimas 3 entradas del punto $\\mathbf{x}_k$ que devuelve el algoritmo,\n",
    "- la norma del vector gradiente $\\mathbf{g}_k$, \n",
    "- la variable $bres$ para saber si el algoritmo puedo converger.\n",
    "- el número de reinicios $nr$.\n",
    "\n",
    "5. Repita las pruebas usando el algoritmo de descenso máximo con los mismos\n",
    "   parámetros y usando backtracking con las condiciones de Wolfe.\n",
    "   \n",
    "6. Compare los resultados con ambos métodos y escriba un comentario sobre el \n",
    "   desempeño de ambos algoritmos. \n",
    "  \n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradConjugado_noLineal (x0, fun, grad_f, N=50000, tau = eps, a_ini=1, rho=0.75, c1=0.001, c2=0.9, Nb=100):\n",
    "    if tau <= 0:\n",
    "        raise Exception(\"Valor incorrecto para tau\")\n",
    "    \n",
    "    xk = np.asarray(x0)\n",
    "\n",
    "    delta = eps**(2/3)\n",
    "    n = len(x0)\n",
    "    nr = 0 \n",
    "    beta_k=0\n",
    "\n",
    "    gk = grad_f(xk)\n",
    "    dk = -gk \n",
    "    for i in range(N):\n",
    "        if np.linalg.norm(gk) < tau:\n",
    "            return xk, gk, i, True, nr\n",
    "        \n",
    "        alpha_k = back_tracking_wolf(fun, grad_f, xk, dk, a_ini)\n",
    "        xk = xk + alpha_k * dk \n",
    "        gk_1 = grad_f(xk)\n",
    "        yk = gk_1 - gk\n",
    "\n",
    "        denom = np.dot(dk, yk)\n",
    "\n",
    "        if i>0 and i%n > 0 and abs(denom) > delta:\n",
    "            beta_k = (gk_1.T @ yk)/denom \n",
    "        else:\n",
    "            beta_k = 0\n",
    "            nr += 1\n",
    "\n",
    "        dk = -gk_1 + beta_k * dk\n",
    "\n",
    "        if dk.T @ gk_1 >= 0: \n",
    "            dk = - gk_1\n",
    "\n",
    "        gk = gk_1.copy()\n",
    "\n",
    "\n",
    "    return xk, gk, N, False, nr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas del algoritmo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas para la funcion cuadratica para n = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "Resultados de la funcion cuadratica con n = 101\n"
     ]
    }
   ],
   "source": [
    "n = 101\n",
    "diagP = np.full(n, 2)\n",
    "diagS = np.full(n-1, -1)\n",
    "diagI = np.full(n-1, -1)\n",
    "\n",
    "A = np.diag(diagP, k=0)\n",
    "A += np.diag(diagS, k=1)\n",
    "A += np.diag(diagS, k=-1)\n",
    "\n",
    "b = np.zeros(n)\n",
    "b[0] = 1\n",
    "b[-1] = 1\n",
    "\n",
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "\n",
    "def fun_ob(x):\n",
    "    xA = np.dot(x, A)\n",
    "    return 1/2 * np.dot(xA, x) - np.dot(b, x) \n",
    "\n",
    "def grad_fun_obj(x):\n",
    "    return np.dot(A,x) - b\n",
    "\n",
    "x0 = b.copy()\n",
    "\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"Resultados de la funcion cuadratica con n = 101\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para funcion cuadratica con x0: [1. 0. 0.]   [0. 0. 1.]\n",
      "Iterciones completadas:  5669\n",
      "xk: [' 1.0000', ' 1.0000', ' 1.0000']  [' 1.0000', ' 1.0000', ' 1.0000']\n",
      "f(xk): -1.0\n",
      "f(x0):  0.0\n",
      "||f(xk)||:  1.0\n",
      "||f'(xk)||:  1.362e-07\n",
      "Se cumplio el criterio de paro:  True\n",
      "Cantidad de reinicios:  5342\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xk, gk, i, flag, cont = gradConjugado_noLineal(x0, fun_ob, grad_fun_obj, tau=tol)\n",
    "\n",
    "f0 = fun_ob(x0)\n",
    "fk = fun_ob(xk)\n",
    "gk = grad_fun_obj(xk)\n",
    "print(\"Resultados para funcion cuadratica con x0:\", x0[:3], \" \", x0[-3:])\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk[:3]]}  {[f'{x: .4f}' for x in xk[-3:]]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "print(\"Cantidad de reinicios: \", cont) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para funcion cuadratica con x0: [1. 0. 0.]   [0. 0. 1.]\n",
      "Iterciones completadas:  24094\n",
      "xk: [' 1.0000', ' 1.0000', ' 1.0000']  [' 1.0000', ' 1.0000', ' 1.0000']\n",
      "f(xk): -1.0\n",
      "f(x0):  0.0\n",
      "||f(xk)||:  1.0\n",
      "||f'(xk)||:  1.489e-07\n",
      "Se cumplio el criterio de paro:  True\n"
     ]
    }
   ],
   "source": [
    "xk, i, flag, _ = desc_max_btWolf(x0=x0, f=fun_ob, grad_f=grad_fun_obj, tol=tol, N=50000)\n",
    "\n",
    "f0 = fun_ob(x0)\n",
    "fk = fun_ob(xk)\n",
    "gk = grad_fun_obj(xk)\n",
    "print(\"Resultados para funcion cuadratica con x0:\", x0[:3], \" \", x0[-3:])\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk[:3]]}  {[f'{x: .4f}' for x in xk[-3:]]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas para la funcion cuadratica para n = 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creacion de la matriz\n",
    "n = 1001\n",
    "diagP = np.full(n, 2)\n",
    "diagS = np.full(n-1, -1)\n",
    "diagI = np.full(n-1, -1)\n",
    "\n",
    "A = np.diag(diagP, k=0)\n",
    "A += np.diag(diagS, k=1)\n",
    "A += np.diag(diagS, k=-1)\n",
    "\n",
    "b = np.zeros(n)\n",
    "b[0] = 1\n",
    "b[-1] = 1\n",
    "\n",
    "tol = (n * eps)**(1/2)\n",
    "x0 = b.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 -1  0 ...  0  0  0]\n",
      " [-1  2 -1 ...  0  0  0]\n",
      " [ 0 -1  2 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  2 -1  0]\n",
      " [ 0  0  0 ... -1  2 -1]\n",
      " [ 0  0  0 ...  0 -1  2]]\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para funcion cuadratica con x0: [1. 0. 0.]   [0. 0. 1.]\n",
      "Iterciones completadas:  50000\n",
      "xk: [' 1.0000', ' 1.0000', ' 1.0000']  [' 1.0000', ' 1.0000', ' 1.0000']\n",
      "f(xk): -1.0\n",
      "f(x0):  0.0\n",
      "||f(xk)||:  1.0\n",
      "||f'(xk)||:  6.878e-07\n",
      "Se cumplio el criterio de paro:  False\n",
      "Cantidad de reinicios:  47139\n"
     ]
    }
   ],
   "source": [
    "#Resultados de gradiente conjugado no lineal\n",
    "\n",
    "\n",
    "xk, gk, i, flag, cont = gradConjugado_noLineal(x0, fun_ob, grad_fun_obj, tau=tol)\n",
    "\n",
    "f0 = fun_ob(x0)\n",
    "fk = fun_ob(xk)\n",
    "gk = grad_fun_obj(xk)\n",
    "print(\"Resultados para funcion cuadratica con x0:\", x0[:3], \" \", x0[-3:])\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk[:3]]}  {[f'{x: .4f}' for x in xk[-3:]]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "print(\"Cantidad de reinicios: \", cont) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para funcion cuadratica con x0: [1. 0. 0.]   [0. 0. 1.]\n",
      "Iterciones completadas:  50000\n",
      "xk: [' 0.9965', ' 0.9929', ' 0.9894']  [' 0.9894', ' 0.9929', ' 0.9965']\n",
      "f(xk): -0.9975\n",
      "f(x0):  0.0\n",
      "||f(xk)||:  0.9975\n",
      "||f'(xk)||:  0.0005949\n",
      "Se cumplio el criterio de paro:  False\n",
      "Cantidad de reinicios:  47139\n"
     ]
    }
   ],
   "source": [
    "#Resultados para descenso maximo de gradiente\n",
    "xk, i, flag, _ = desc_max_btWolf(x0=x0, f=fun_ob, grad_f=grad_fun_obj, tol=tol, N=50000)\n",
    "\n",
    "f0 = fun_ob(x0)\n",
    "fk = fun_ob(xk)\n",
    "gk = grad_fun_obj(\n",
    "    xk)\n",
    "print(\"Resultados para funcion cuadratica con x0:\", x0[:3], \" \", x0[-3:])\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk[:3]]}  {[f'{x: .4f}' for x in xk[-3:]]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "print(\"Cantidad de reinicios: \", cont) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas con la función de Beale para el algoritmo de gradiente conjugado no lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de gradiente conjugado no lineal\n",
      "Resultados para funcion de  Beale con x0: [2.0, 3.0]\n",
      "Iterciones completadas:  802\n",
      "xk: [' 3.0000', ' 0.5000']\n",
      "f(xk):  7.137e-17\n",
      "f(x0):  3.347e+03\n",
      "||f(xk)||:  7.137e-17\n",
      "||f'(xk)||:  2.061e-08\n",
      "Se cumplio el criterio de paro:  True\n",
      "Cantidad de reinicios:  542\n"
     ]
    }
   ],
   "source": [
    "x0 = [2., 3.]\n",
    "n = len(x0) \n",
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "\n",
    "print(\"Resultados con el algoritmo de gradiente conjugado no lineal\")\n",
    "xk, gk, i, flag, cont = gradConjugado_noLineal(x0, fun_Beale, grad_fun_Beale, tau=tol)\n",
    "\n",
    "f0 = fun_Beale(x0)\n",
    "fk = fun_Beale(xk)\n",
    "gk = grad_fun_Beale(xk)\n",
    "print(\"Resultados para funcion de  Beale con x0:\", x0)\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "print(\"Cantidad de reinicios: \", cont) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\n",
      "Resultados para funcion de  Beale con x0: [2.0, 3.0]\n",
      "Iterciones completadas:  1299\n",
      "xk: [' 3.0000', ' 0.5000']\n",
      "f(xk):  7.038e-17\n",
      "f(x0):  3.347e+03\n",
      "||f(xk)||:  7.038e-17\n",
      "||f'(xk)||:  2.07e-08\n",
      "Se cumplio el criterio de paro:  True\n"
     ]
    }
   ],
   "source": [
    "x0 = [2., 3.]\n",
    "n = len(x0) \n",
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "print(\"Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\")\n",
    "xk, i, flag, _ = desc_max_btWolf(x0=x0, f=fun_Beale, grad_f=grad_fun_Beale, tol=tol, N=50000)\n",
    "\n",
    "f0 = fun_Beale(x0)\n",
    "fk = fun_Beale(xk)\n",
    "gk = grad_fun_Beale(xk)\n",
    "print(\"Resultados para funcion de  Beale con x0:\", x0)\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "# print(\"Cantidad de reinicios: \", cont) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas con la función de Himmenlblau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de gradiente conjugado no lineal\n",
      "Resultados para funcion de  Himmenlblau con x0: [2.0, 4.0]\n",
      "Iterciones completadas:  135\n",
      "xk: [' 3.5844', '-1.8481']\n",
      "f(xk):  1.837e-18\n",
      "f(x0):  130.0\n",
      "||f(xk)||:  1.837e-18\n",
      "||f'(xk)||:  1.968e-08\n",
      "Se cumplio el criterio de paro:  True\n",
      "Cantidad de reinicios:  89\n"
     ]
    }
   ],
   "source": [
    "x0 = [2., 4.]\n",
    "n = len(x0) \n",
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "xk, gk, i, flag, cont = gradConjugado_noLineal(x0, fun_Himmenlblau, grad_Himmenlblau, tau=tol)\n",
    "\n",
    "print(\"Resultados con el algoritmo de gradiente conjugado no lineal\")\n",
    "\n",
    "\n",
    "f0 = fun_Himmenlblau(x0)\n",
    "fk = fun_Himmenlblau(xk)\n",
    "gk = grad_Himmenlblau(xk) \n",
    "print(\"Resultados para funcion de  Himmenlblau con x0:\", x0)\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "print(\"Cantidad de reinicios: \", cont) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\n",
      "Resultados para funcion de  Himmenlblau con x0: [2.0, 4.0]\n",
      "Iterciones completadas:  168\n",
      "xk: [' 3.5844', '-1.8481']\n",
      "f(xk):  1.9e-18\n",
      "f(x0):  130.0\n",
      "||f(xk)||:  1.9e-18\n",
      "||f'(xk)||:  2.001e-08\n",
      "Se cumplio el criterio de paro:  True\n"
     ]
    }
   ],
   "source": [
    "x0 = [2., 4.]\n",
    "n = len(x0) \n",
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "print(\"Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\")\n",
    "xk, i, flag, _ = desc_max_btWolf(x0=x0, f=fun_Himmenlblau, grad_f=grad_Himmenlblau, tol=tol, N=50000)\n",
    "\n",
    "f0 = fun_Himmenlblau(x0)\n",
    "fk = fun_Himmenlblau(xk)\n",
    "gk = grad_Himmenlblau(xk)\n",
    "print(\"Resultados para funcion de  Himmenlblau con x0:\", x0)\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "# print(\"Cantidad de reinicios: \", cont) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de gradiente conjugado no lineal\n",
      "Resultados para funcion de  Himmenlblau con x0: [0.0, 0.0]\n",
      "Iterciones completadas:  634\n",
      "xk: [' 3.0000', ' 2.0000']\n",
      "f(xk):  0.0\n",
      "f(x0):  170.0\n",
      "||f(xk)||:  0.0\n",
      "||f'(xk)||:  0.0\n",
      "Se cumplio el criterio de paro:  True\n",
      "Cantidad de reinicios:  489\n"
     ]
    }
   ],
   "source": [
    "x0 = [0., 0.]\n",
    "n = len(x0) \n",
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "xk, gk, i, flag, cont = gradConjugado_noLineal(x0, fun_Himmenlblau, grad_Himmenlblau)\n",
    "\n",
    "print(\"Resultados con el algoritmo de gradiente conjugado no lineal\")\n",
    "\n",
    "\n",
    "f0 = fun_Himmenlblau(x0)\n",
    "fk = fun_Himmenlblau(xk)\n",
    "gk = grad_Himmenlblau(xk) \n",
    "print(\"Resultados para funcion de  Himmenlblau con x0:\", x0)\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "print(\"Cantidad de reinicios: \", cont) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\n",
      "Resultados para funcion de  Himmenlblau con x0: [0.0, 0.0]\n",
      "Iterciones completadas:  99\n",
      "xk: ['-3.7793', '-3.2832']\n",
      "f(xk):  1.205e-18\n",
      "f(x0):  170.0\n",
      "||f(xk)||:  1.205e-18\n",
      "||f'(xk)||:  1.796e-08\n",
      "Se cumplio el criterio de paro:  True\n"
     ]
    }
   ],
   "source": [
    "x0 = [0., 0.]\n",
    "n = len(x0) \n",
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "print(\"Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\")\n",
    "xk, i, flag, _ = desc_max_btWolf(x0=x0, f=fun_Himmenlblau, grad_f=grad_Himmenlblau, tol=tol, N=50000)\n",
    "\n",
    "f0 = fun_Himmenlblau(x0)\n",
    "fk = fun_Himmenlblau(xk)\n",
    "gk = grad_Himmenlblau(xk)\n",
    "print(\"Resultados para funcion de  Himmenlblau con x0:\", x0)\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "# print(\"Cantidad de reinicios: \", cont) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas con la función de Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de gradiente conjugado no lineal\n",
      "Resultados para funcion de  Rosenbrock con x0: [-1.2, 1.0]\n",
      "Iterciones completadas:  50000\n",
      "xk: [' 1.0000', ' 1.0000']\n",
      "f(xk):  3.905e-28\n",
      "f(x0):  24.2\n",
      "||f(xk)||:  3.905e-28\n",
      "||f'(xk)||:  3.952e-14\n",
      "Se cumplio el criterio de paro:  False\n",
      "Cantidad de reinicios:  49932\n"
     ]
    }
   ],
   "source": [
    "x0 = [-1.2, 1.]\n",
    "n = len(x0) \n",
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "xk, gk, i, flag, cont = gradConjugado_noLineal(x0, fun_Rosenbrock, grad_fun_Rosenbrock)\n",
    "\n",
    "print(\"Resultados con el algoritmo de gradiente conjugado no lineal\")\n",
    "\n",
    "\n",
    "f0 = fun_Rosenbrock(x0)\n",
    "fk = fun_Rosenbrock(xk)\n",
    "gk = grad_fun_Rosenbrock(xk) \n",
    "print(\"Resultados para funcion de  Rosenbrock con x0:\", x0)\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "print(\"Cantidad de reinicios: \", cont) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\n",
      "Resultados para funcion de  Rosenbock con x0: [-1.2, 1.0]\n",
      "Iterciones completadas:  24052\n",
      "xk: [' 1.0000', ' 1.0000']\n",
      "f(xk):  1.241e-16\n",
      "f(x0):  24.2\n",
      "||f(xk)||:  1.241e-16\n",
      "||f'(xk)||:  2.068e-08\n",
      "Se cumplio el criterio de paro:  True\n"
     ]
    }
   ],
   "source": [
    "x0 = [-1.2, 1.]\n",
    "n = len(x0) \n",
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "print(\"Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\")\n",
    "xk, i, flag, _ = desc_max_btWolf(x0=x0, f=fun_Rosenbrock, grad_f=grad_fun_Rosenbrock, tol=tol, N=50000)\n",
    "\n",
    "f0 = fun_Rosenbrock(x0)\n",
    "fk = fun_Rosenbrock(xk)\n",
    "gk = grad_fun_Rosenbrock(xk)\n",
    "print(\"Resultados para funcion cuadratica con x0:\", x0[:3], \" \", x0[-3:])\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "# print(\"Cantidad de reinicios: \", cont) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas con $x \\in \\mathbb R^{20}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "\n",
    "x0 = np.zeros(n)\n",
    "\n",
    "for i in range(n):\n",
    "    if i %2 == 0: \n",
    "        x0[i] = -1.2\n",
    "    else: \n",
    "        x0[i] = 1.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de gradiente conjugado no lineal\n",
      "Resultados para funcion cuadratica con x0: [-1.2  1.  -1.2]   [ 1.  -1.2  1. ]\n",
      "Iterciones completadas:  50000\n",
      "xk: [' 1.0879', ' 1.0137', ' 1.0429']  ['-0.0328', ' 0.8596', ' 0.8617']\n",
      "f(xk):  104.8\n",
      "f(x0):  4.598e+03\n",
      "||f(xk)||:  104.8\n",
      "||f'(xk)||:  211.5\n",
      "Se cumplio el criterio de paro:  False\n",
      "Cantidad de reinicios:  2500\n"
     ]
    }
   ],
   "source": [
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "xk, gk, i, flag, cont = gradConjugado_noLineal(x0, fun_Rosenbrock, grad_fun_Rosenbrock)\n",
    "\n",
    "print(\"Resultados con el algoritmo de gradiente conjugado no lineal\")\n",
    "\n",
    "\n",
    "f0 = fun_Rosenbrock(x0)\n",
    "fk = fun_Rosenbrock(xk)\n",
    "gk = grad_fun_Rosenbrock(xk) \n",
    "print(\"Resultados para funcion cuadratica con x0:\", x0[:3], \" \", x0[-3:])\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk[:3]]}  {[f'{x: .4f}' for x in xk[-3:]]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "print(\"Cantidad de reinicios: \", cont) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\n",
      "Resultados para funcion cuadratica con x0: [-1.2  1.  -1.2]   [ 1.  -1.2  1. ]\n",
      "Iterciones completadas:  50000\n",
      "xk: [' 0.9338', ' 0.9079', ' 0.9062']  ['-0.7498', ' 0.8799', ' 0.7037']\n",
      "f(xk):  127.8\n",
      "f(x0):  4.598e+03\n",
      "||f(xk)||:  127.8\n",
      "||f'(xk)||:  166.4\n",
      "Se cumplio el criterio de paro:  False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\")\n",
    "xk, i, flag, _ = desc_max_btWolf(x0=x0, f=fun_Rosenbrock, grad_f=grad_fun_Rosenbrock, tol=tol, N=50000)\n",
    "\n",
    "f0 = fun_Rosenbrock(x0)\n",
    "fk = fun_Rosenbrock(xk)\n",
    "gk = grad_fun_Rosenbrock(xk)\n",
    "print(\"Resultados para funcion cuadratica con x0:\", x0[:3], \" \", x0[-3:])\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk[:3]]}  {[f'{x: .4f}' for x in xk[-3:]]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "# print(\"Cantidad de reinicios: \", cont) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebas con $x \\in \\mathbb R^{40}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de gradiente conjugado no lineal\n",
      "Resultados para funcion cuadratica con x0: [-1.2  1.  -1.2]   [ 1.  -1.2  1. ]\n",
      "Iterciones completadas:  50000\n",
      "xk: [' 1.0879', ' 1.0137', ' 1.0429']  ['-0.0328', ' 0.8596', ' 0.8617']\n",
      "f(xk):  104.8\n",
      "f(x0):  4.598e+03\n",
      "||f(xk)||:  104.8\n",
      "||f'(xk)||:  211.5\n",
      "Se cumplio el criterio de paro:  False\n",
      "Cantidad de reinicios:  2500\n"
     ]
    }
   ],
   "source": [
    "tol = (n * eps)**(1/2)\n",
    "\n",
    "xk, gk, i, flag, cont = gradConjugado_noLineal(x0, fun_Rosenbrock, grad_fun_Rosenbrock)\n",
    "\n",
    "print(\"Resultados con el algoritmo de gradiente conjugado no lineal\")\n",
    "\n",
    "\n",
    "f0 = fun_Rosenbrock(x0)\n",
    "fk = fun_Rosenbrock(xk)\n",
    "gk = grad_fun_Rosenbrock(xk) \n",
    "print(\"Resultados para funcion cuadratica con x0:\", x0[:3], \" \", x0[-3:])\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk[:3]]}  {[f'{x: .4f}' for x in xk[-3:]]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "print(\"Cantidad de reinicios: \", cont) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\n",
      "Resultados para funcion cuadratica con x0: [-1.2  1.  -1.2]   [ 1.  -1.2  1. ]\n",
      "Iterciones completadas:  50000\n",
      "xk: [' 0.9338', ' 0.9079', ' 0.9062']  ['-0.7498', ' 0.8799', ' 0.7037']\n",
      "f(xk):  127.8\n",
      "f(x0):  4.598e+03\n",
      "||f(xk)||:  127.8\n",
      "||f'(xk)||:  166.4\n",
      "Se cumplio el criterio de paro:  False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Resultados con el algoritmo de descenso maximo de gradiente con el backtracking y las condiciones de wolf\")\n",
    "xk, i, flag, _ = desc_max_btWolf(x0=x0, f=fun_Rosenbrock, grad_f=grad_fun_Rosenbrock, tol=tol, N=50000)\n",
    "\n",
    "f0 = fun_Rosenbrock(x0)\n",
    "fk = fun_Rosenbrock(xk)\n",
    "gk = grad_fun_Rosenbrock(xk)\n",
    "print(\"Resultados para funcion cuadratica con x0:\", x0[:3], \" \", x0[-3:])\n",
    "print(\"Iterciones completadas: \", i)\n",
    "print(f\"xk: {[f'{x: .4f}' for x in xk[:3]]}  {[f'{x: .4f}' for x in xk[-3:]]}\")\n",
    "print(f\"f(xk): {fk: .4}\")\n",
    "print(f\"f(x0): {f0: .4}\")\n",
    "print(f\"||f(xk)||: {np.linalg.norm(fk): .4}\")  \n",
    "print(f\"||f'(xk)||: {np.linalg.norm(gk): .4}\")  \n",
    "print(\"Se cumplio el criterio de paro: \", flag)\n",
    "# print(\"Cantidad de reinicios: \", cont) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentarios comparativos entre las ejecuciones con los diferentes algoritmos. \n",
    "\n",
    "En general podemos observar que el algoritmo de descenso conjugado se comporta mejor en cuanto a iteraciones, en el primer caso de la funcion cuadratica con $n = 101$ toma casi cinco veces las iteraciones utilizar el algoritmo de descenso máximo de gradiente. \n",
    "\n",
    "En algunos casos, la implementación de gradiente conjugado no converge, a pesar de esto, observemos como la iteración continua lo hace obtener resultados del puntos óptimo igual o mejor al del algoritmo de máximo descenso. \n",
    "\n",
    "Comparando las normas de las funciones y sus gradientes, evaluados en el punto óptimo devuelto por el algoritmo, nos damos cuenta que el algoritmo de descenso conjugado no lineal optiene puntos más pequeños. Esto es más evidente cuando ninguno de los algoritmos converge, pues en la misma cantidad de iteraciones se obtienen mejores resultados.  \n",
    "\n",
    "Incluso en la ejecución de la función Himmenlblau, inciando en (2, 4), ambos algoritmos obtienen un resultado diferente al esperado,  aunque la norma del gradiente en este punto en 0, podemos ver que al inciar en el punto (0, 0) llegamos al óptimo alcanzado por otros algoritmos en tareas pasadas, lo que nos permite intuir que este algoritmo, con la condición de Wolf para backtrackin es suceptible a atorarse en mínimos locales. \n",
    "\n",
    "Ambos algoritmos resultan ser bastante sensibles al aumento de dimensión, podemos observar esto en el tiempo de ejecución. En las pruebas con la función de Rosenbrock. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2 (5 puntos)\n",
    "\n",
    "Construir un clasificador binario basado en el método de regresión logística.\n",
    "Revise las notas en la clase 14 sobre \"Regresión logística binaria\".\n",
    "\n",
    "Tenemos un conjunto de datos y cada dato puede pertener a una de dos clases. \n",
    "Las clases se identifican con las etiquetas \"-1\" y \"1\". Para hacer la clasificación\n",
    "se necesita determinar un vector $\\mathbf{w}$ que se usa para\n",
    "calcular la probabilidad de que un dato $\\mathbf{x}_i \\in \\mathbb{R}^n$ \n",
    "pertenezca a la clase $y_i=1$ mediante la evaluación de la función de regresión logística:\n",
    "\n",
    "$$ LR(\\mathbf{x}_i; \\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^\\top\\mathbf{x}_i)}.  $$ \n",
    "\n",
    "\n",
    "1. Programar las función $LR(\\mathbf{x}; \\mathbf{w})$, la función objetivo\n",
    "\n",
    "$$\n",
    "f(\\mathbf{w}) \n",
    "= \\sum_{i=1}^m \\log \\left(1 + \\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i) \\right) + \n",
    "\\frac{\\lambda}{2}\\mathbf{w}^\\top\\mathbf{w},\n",
    "$$\n",
    "\n",
    "y su gradiente \n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{w}) \n",
    "=  \\lambda \\mathbf{w} - \\sum_{i=1}^m \\left[1 -  h(\\mathbf{x}_i, y_i; \\,\\mathbf{w})\\right]  y_i \\mathbf{x}_i.\n",
    "$$\n",
    "\n",
    "(Página 28 de las diapositivas de la clase 14)\n",
    "\n",
    "2. Descargue el archivo CSV, ejecute las celdas en donde se cargan los datos del problema\n",
    "   y se crean los conjuntos de entrenamiento `X_train` y `y_train` y los \n",
    "   datos del conjunto de prueba `X_test` y `y_test`.\n",
    "   \n",
    "3. Aplique el método de descenso máximo para minimizar la función $f(\\mathbf{w})$ usando \n",
    "   los datos en el conjunto de entrenamiento: `X_train` y `y_train`. \n",
    "\n",
    "- El dato $\\mathbf{x}_i$ es la fila i-ésima de la matriz `X_train` y su etiqueta\n",
    "  es la componente i-ésima del arreglo `y_train`.\n",
    "- Use backtracking con las condiciones de Wolfe para calcular el tamaño de paso $\\alpha_k$.\n",
    "- Fije el máximo de iteraciones $N=50000$, $\\tau = \\sqrt{n}\\epsilon_m^{1/3}$, donde $n$ es la dimensión   \n",
    "  de la variable $\\mathbf{w}$ y $\\epsilon_m$ es el épsilon máquina. \n",
    "- Para backtracking use $a_{ini}=1$, $\\rho=0.75$, $c_1=0.001$, $c_2=0.9$ y $N_b=100$ iteraciones máximas.\n",
    "- Dé como punto inicial $\\mathbf{w}_0 = (-1, -1, ..., -1)$ y $\\lambda = 0.001$.\n",
    "  \n",
    "4. Una vez que se ha calculado el minimizador $\\mathbf{w}_{*}$ de $f(\\mathbf{w})$\n",
    "   use la función $LR(\\mathbf{x}; \\mathbf{w}_{*})$ para calcular un\n",
    "   arreglo que contenga la probabilidades de que los datos en el conjunto\n",
    "   de prueba `X_test` pertenezcan a la clase $1$. Luego use \n",
    "   la función $predict()$ para predecir las etiquetas de los datos a partir\n",
    "   del arreglo de probabilidades. Finalmente use el función `recall_score`\n",
    "   de la librería Scikit Learn para obtener la medida de sensibilidad, que muestra\n",
    "   el desempeño que tiene el algoritmo.\n",
    "   \n",
    "5. Repita los pasos 3 y 4 usando $\\lambda=1.0$\n",
    "6. Repita los pasos 3, 4 y 5 usando el algoritmo de gradiente conjugado no lineal.\n",
    "   \n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos\n",
    "\n",
    "\n",
    "Descargue el archivo `clasif01_glaucoma.csv` y ejecute las siguientes líneas de código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import time \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix, recall_score\n",
    "\n",
    "# Cargamos los datos \n",
    "dfX = pd.read_csv(\"clasif01_glaucoma.csv\")\n",
    "\n",
    "# Extraer la variable respuesta\n",
    "dfY = dfX.pop('glaucoma')\n",
    "y = dfY.to_numpy()\n",
    "\n",
    "# Cambiamos las etiquetes 0 por -1\n",
    "ii     = np.where(y==0)[0]\n",
    "y[ii]  = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>age</th>\n",
       "      <th>ocular_pressure</th>\n",
       "      <th>MD</th>\n",
       "      <th>PSD</th>\n",
       "      <th>GHT</th>\n",
       "      <th>cornea_thickness</th>\n",
       "      <th>RNFL4.mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.334851</td>\n",
       "      <td>-0.464246</td>\n",
       "      <td>0.789514</td>\n",
       "      <td>-0.869371</td>\n",
       "      <td>-1.343090</td>\n",
       "      <td>0.527140</td>\n",
       "      <td>0.806925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.334851</td>\n",
       "      <td>-0.464246</td>\n",
       "      <td>0.779793</td>\n",
       "      <td>-0.970349</td>\n",
       "      <td>-1.343090</td>\n",
       "      <td>0.705919</td>\n",
       "      <td>0.975183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.594711</td>\n",
       "      <td>-1.049835</td>\n",
       "      <td>0.681606</td>\n",
       "      <td>-0.615751</td>\n",
       "      <td>0.808441</td>\n",
       "      <td>-1.499017</td>\n",
       "      <td>3.084885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.594711</td>\n",
       "      <td>-1.049835</td>\n",
       "      <td>0.731185</td>\n",
       "      <td>-0.383267</td>\n",
       "      <td>0.808441</td>\n",
       "      <td>-1.350035</td>\n",
       "      <td>0.638666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.249836</td>\n",
       "      <td>0.355578</td>\n",
       "      <td>0.560088</td>\n",
       "      <td>-0.406750</td>\n",
       "      <td>0.808441</td>\n",
       "      <td>0.199379</td>\n",
       "      <td>-0.306170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x0       age  ocular_pressure        MD       PSD       GHT  \\\n",
       "0  1.0  0.334851        -0.464246  0.789514 -0.869371 -1.343090   \n",
       "1  1.0  0.334851        -0.464246  0.779793 -0.970349 -1.343090   \n",
       "2  1.0  0.594711        -1.049835  0.681606 -0.615751  0.808441   \n",
       "3  1.0  0.594711        -1.049835  0.731185 -0.383267  0.808441   \n",
       "4  1.0 -0.249836         0.355578  0.560088 -0.406750  0.808441   \n",
       "\n",
       "   cornea_thickness  RNFL4.mean  \n",
       "0          0.527140    0.806925  \n",
       "1          0.705919    0.975183  \n",
       "2         -1.499017    3.084885  \n",
       "3         -1.350035    0.638666  \n",
       "4          0.199379   -0.306170  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizamos las variables\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dfX)\n",
    "dfX = pd.DataFrame(scaler.transform(dfX),columns=dfX.columns)\n",
    "\n",
    "# Se agrega la columna con 1's\n",
    "dfX.insert(0, \"x0\", np.ones(len(dfX)))\n",
    "dfX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros para entrenamiento: 399\n",
      "Número de registros para prueba: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>age</th>\n",
       "      <th>ocular_pressure</th>\n",
       "      <th>MD</th>\n",
       "      <th>PSD</th>\n",
       "      <th>GHT</th>\n",
       "      <th>cornea_thickness</th>\n",
       "      <th>RNFL4.mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.958980</td>\n",
       "      <td>-0.698482</td>\n",
       "      <td>0.221783</td>\n",
       "      <td>-0.085029</td>\n",
       "      <td>0.808441</td>\n",
       "      <td>0.020601</td>\n",
       "      <td>0.561008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.444732</td>\n",
       "      <td>0.941167</td>\n",
       "      <td>-0.830075</td>\n",
       "      <td>1.906353</td>\n",
       "      <td>0.808441</td>\n",
       "      <td>-1.499017</td>\n",
       "      <td>0.418636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.444732</td>\n",
       "      <td>-0.698482</td>\n",
       "      <td>0.869230</td>\n",
       "      <td>-0.777786</td>\n",
       "      <td>-1.343090</td>\n",
       "      <td>0.765511</td>\n",
       "      <td>1.402301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.379766</td>\n",
       "      <td>-0.581364</td>\n",
       "      <td>0.853675</td>\n",
       "      <td>-0.921034</td>\n",
       "      <td>-1.343090</td>\n",
       "      <td>1.510422</td>\n",
       "      <td>0.975183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.679070</td>\n",
       "      <td>-0.815600</td>\n",
       "      <td>0.724380</td>\n",
       "      <td>-0.867023</td>\n",
       "      <td>-1.343090</td>\n",
       "      <td>1.778590</td>\n",
       "      <td>1.790589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x0       age  ocular_pressure        MD       PSD       GHT  \\\n",
       "95   1.0  1.958980        -0.698482  0.221783 -0.085029  0.808441   \n",
       "54   1.0 -0.444732         0.941167 -0.830075  1.906353  0.808441   \n",
       "456  1.0 -0.444732        -0.698482  0.869230 -0.777786 -1.343090   \n",
       "447  1.0 -0.379766        -0.581364  0.853675 -0.921034 -1.343090   \n",
       "345  1.0 -1.679070        -0.815600  0.724380 -0.867023 -1.343090   \n",
       "\n",
       "     cornea_thickness  RNFL4.mean  \n",
       "95           0.020601    0.561008  \n",
       "54          -1.499017    0.418636  \n",
       "456          0.765511    1.402301  \n",
       "447          1.510422    0.975183  \n",
       "345          1.778590    1.790589  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfX, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print('Número de registros para entrenamiento:', len(y_train))\n",
    "print('Número de registros para prueba:', len(y_test))\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que recibe un arreglo de probabilidades de pertenencia de los\n",
    "# datos a la clase 1 y devuelve un arreglo con las etiquetas de la clase\n",
    "# que se le asigna a los datos.\n",
    "def predict(vprob):  \n",
    "    # Vector de prediciones de las etiquita. Se inicializa como si todas las etiquetas fueran 1\n",
    "    y_pred = np.ones(len(vprob))\n",
    "    # Se obtienen los índices de los datos que tienen una probabilidad menor a 0.5\n",
    "    ii     = np.where(vprob<=0.5)[0]\n",
    "    # Se cambia la etiqueta por -1 para todos los datos con probabilidad menor a 0.5\n",
    "    y_pred[ii] = -1\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos el dataframe que contiene los datos de entrenamiento a una matriz de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las funciones de regresión logistica, la función objetivo y su gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg (x, w):\n",
    "    \n",
    "    if len(x) != len(w):\n",
    "        raise Exception(\"Las dimensiones de x y w no coinciden\")\n",
    "\n",
    "    \n",
    "    return 1/(1+ np.exp(-np.dot(w, x)))\n",
    "\n",
    "def funcion_obj(w, X, y, lambda_):\n",
    "    m = len(y)\n",
    "\n",
    "    suma = 0.0\n",
    "    for i in range(m):\n",
    "        suma+= np.log(1 + np.exp(-y[i] * np.dot(X[i, :], w)))\n",
    "\n",
    "    reg = (lambda_ /2) * np.dot(w, w) \n",
    "\n",
    "    return suma + reg\n",
    "\n",
    "def grad_fun_obj (w, X, y, lambda_):\n",
    "    m = len(y)\n",
    "    w = np.asarray(w)\n",
    "    suma = lambda_*w\n",
    "\n",
    "    for i in range(m):\n",
    "        yi = np.asarray(y[i])\n",
    "        suma -= (1 - log_reg(X[i,:], yi * w)) * yi * X[i, :]\n",
    "\n",
    "    return suma\n",
    "\n",
    "def f(w):\n",
    "    return funcion_obj(w, X_train, y_train, lambda_)\n",
    "\n",
    "def grad_f(w):\n",
    "    return grad_fun_obj(w, X_train, y_train, lambda_)\n",
    "\n",
    "lambda_ = 0.0001\n",
    "\n",
    "def predict(X, w):\n",
    "    prob = 1 / (1 + np.exp(-np.dot(X, w)))\n",
    "    return np.where(prob >= 0.5, 1, -1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo con el metodo de gradiente conjugado para $\\lambda = 0.0001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de w optimos: [' 2.8061', ' 0.7033', ' 1.4971', ' 1.1446', ' 3.6942', ' 0.0528', '-0.6201', '-3.1398']\n",
      "Se cumplio el criterio de paro: True\n",
      "Numero de iteraciones completadas:  191.0000\n"
     ]
    }
   ],
   "source": [
    "w_opt, grad_w, n_it, flag, nr = gradConjugado_noLineal(\n",
    "    x0=-np.ones(X_train.shape[1]),\n",
    "    fun=f, \n",
    "    grad_f=grad_f, \n",
    "    tau=np.sqrt(X_train.shape[1]) * eps**(1/3)\n",
    ")\n",
    "\n",
    "print(f\"Valores de w optimos: {[f'{x: .4f}' for x in w_opt]}\")\n",
    "print(f\"Se cumplio el criterio de paro: {flag}\")\n",
    "print(f\"Numero de iteraciones completadas: {n_it: .4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.8448\n"
     ]
    }
   ],
   "source": [
    "def new_predict(X, w):\n",
    "    prob = 1 / (1 + np.exp(-np.dot(X, w)))\n",
    "    return predict(prob) \n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall: {recall: .4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repitamos el entrenamiento con $\\lambda = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para lambda=1 con descenso conjugado de gradiente\n",
      "Valores de w optimos: [' 1.7589', ' 0.5696', ' 1.1453', ' 0.2983', ' 1.7606', ' 0.4300', '-0.4662', '-2.3293']\n",
      "Se cumplio el criterio de paro: True\n",
      "Numero de iteraciones completadas:  90.0000\n",
      "Recall:  0.8448\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 1 \n",
    "\n",
    "print(\"Resultados para lambda=1 con descenso conjugado de gradiente\")\n",
    "w_opt, grad_w, n_it, flag, nr = gradConjugado_noLineal(\n",
    "    x0=-np.ones(X_train.shape[1]),\n",
    "    fun=f, \n",
    "    grad_f=grad_f, \n",
    "    tau=np.sqrt(X_train.shape[1]) * eps**(1/3)\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Valores de w optimos: {[f'{x: .4f}' for x in w_opt]}\")\n",
    "print(f\"Se cumplio el criterio de paro: {flag}\")\n",
    "print(f\"Numero de iteraciones completadas: {n_it: .4f}\")\n",
    "\n",
    "y_pred = new_predict(X_test, w_opt)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall: {recall: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, realizemos los mismos experimentos con el descenso maximo de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para lambda=0.0001 con descenso maximo de gradiente\n",
      "Valores de w optimos: [' 2.8061', ' 0.7033', ' 1.4971', ' 1.1446', ' 3.6942', ' 0.0528', '-0.6201', '-3.1398']\n",
      "Se cumplio el criterio de paro: True\n",
      "Numero de iteraciones completadas:  496.0000\n",
      "Recall:  0.8448\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.0001 \n",
    "\n",
    "print(\"Resultados para lambda=0.0001 con descenso maximo de gradiente\")\n",
    "\n",
    "w_opt, n_it, flag, _ = desc_max_btWolf(\n",
    "    x0=-np.ones(X_train.shape[1]),\n",
    "    f=f, \n",
    "    grad_f=grad_f, \n",
    "    tol=np.sqrt(X_train.shape[1]) * eps**(1/3)\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Valores de w optimos: {[f'{x: .4f}' for x in w_opt]}\")\n",
    "print(f\"Se cumplio el criterio de paro: {flag}\")\n",
    "print(f\"Numero de iteraciones completadas: {n_it: .4f}\")\n",
    "\n",
    "y_pred = new_predict(X_test, w_opt)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall: {recall: .4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para lambda=1 con descenso maximo de gradiente\n",
      "Valores de w optimos: [' 1.7589', ' 0.5696', ' 1.1453', ' 0.2983', ' 1.7606', ' 0.4300', '-0.4662', '-2.3293']\n",
      "Se cumplio el criterio de paro: True\n",
      "Numero de iteraciones completadas:  116.0000\n",
      "Recall:  0.8448\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados para lambda=1 con descenso maximo de gradiente\")\n",
    "\n",
    "lambda_ = 1\n",
    "\n",
    "w_opt, n_it, flag, _ = desc_max_btWolf(\n",
    "    x0=-np.ones(X_train.shape[1]),\n",
    "    f=f, \n",
    "    grad_f=grad_f, \n",
    "    tol=np.sqrt(X_train.shape[1]) * eps**(1/3)\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Valores de w optimos: {[f'{x: .4f}' for x in w_opt]}\")\n",
    "print(f\"Se cumplio el criterio de paro: {flag}\")\n",
    "print(f\"Numero de iteraciones completadas: {n_it: .4f}\")\n",
    "\n",
    "y_pred = new_predict(X_test, w_opt)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Recall: {recall: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos que el vector $w$ óptimo resulta diferente para $\\lambda = 1$ y $\\lambda = 0.0001$. A pesar de ser diferentes, obtenemos el mismo valor en la función que mide el puntaje de recall. También podemos observar que ambos algoritmos con $\\lambda = 1$ obtienen el mismo valor para $w$ que la función que se utiliza en el archivo de la ayudantía 5. \n",
    "\n",
    "Por otro lado, observamos que el score obtenido es menor al obtenido por la libreria de scikit-learn. \n",
    "\n",
    "Comparando el desempeño de los algoritmos, en general, podemos que el algoritmo de descenso conjugado realiza menos iteraciones. Estos es más apreciable cuando $\\lambda = 0.0001$ pues el algoritmo de descenso maximo realiza más del doble de iteraciones. \n",
    "\n",
    "Concluyendo que el algoritmo de descenso conjugado no lineal tiene un mejor desempeño. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tareas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
